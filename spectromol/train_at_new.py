import os
import torch
import torch.nn as nn
import torch.utils.data as data
import numpy as np
import pandas as pd
from tqdm import tqdm
from model import *
from dataset import *
from sklearn.preprocessing import StandardScaler
import csv
from rdkit import Chem, RDLogger, DataStructs
from rdkit.Chem import AllChem, MACCSkeys
from Levenshtein import distance as lev
RDLogger.DisableLog('rdApp.*')

# import pickle
# path = '/root/workspace/smiles-transformer-master/csv/output_smiles_vectors.pkl'
# with open(path, 'rb') as f: 
#     atom_coor_smiles = pickle.load(f)
#     import pdb;pdb.set_trace()
#     print('a')


# from sm_trans.smiles_transformer.build_vocab import WordVocab
# from sm_trans.smiles_transformer.pretrain_trfm import TrfmSeq2seq

# vocab = WordVocab.load_vocab('/root/workspace/smiles-transformer-master/csv/vocab.pkl')
# trfm = TrfmSeq2seq(len(vocab), 256, len(vocab), 4)
# trfm.load_state_dict(torch.load('/root/workspace/smiles-transformer-master/csv/trfm_12_23000.pkl'))
# trfm.eval()

device = 'cuda'

data_split_mode = 'scaffold'


random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)



from sklearn.metrics import accuracy_score, mean_absolute_error
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import torch.utils.data as data


from collections import Counter
import math


class SemanticSupervisedSMILESLoss(nn.Module):
    """
    ç®€åŒ–çš„SMILESæŸå¤±å‡½æ•°ï¼Œä¸»è¦ä½¿ç”¨å®˜èƒ½å›¢æƒ©ç½šè¿›è¡Œè¯­ä¹‰ç›‘ç£
    """
    def __init__(self, ignore_index, functional_penalty_weight=0.05):
        super(SemanticSupervisedSMILESLoss, self).__init__()
        self.smiles_loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)
        
        # å®˜èƒ½å›¢æƒ©ç½šæƒé‡ - è¾ƒå°çš„æƒé‡é¿å…å½±å“ä¸»ä»»åŠ¡ç¨³å®šæ€§
        self.functional_penalty_weight = functional_penalty_weight
    
    def forward(self, smiles_output, smiles_target, auxiliary_targets, count_task_outputs, 
                binary_task_outputs, count_tasks, binary_tasks):
        """
        è®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬SMILESæŸå¤±å’Œè¾…åŠ©ä»»åŠ¡æŸå¤±
        
        Args:
            smiles_output: [seq_len * batch_size, vocab_size] - SMILESé¢„æµ‹è¾“å‡º
            smiles_target: [seq_len * batch_size] - SMILESçœŸå®æ ‡ç­¾
            auxiliary_targets: [batch_size, num_aux_tasks] - è¾…åŠ©ä»»åŠ¡çœŸå®æ ‡ç­¾
            count_task_outputs: dict - è®¡æ•°ä»»åŠ¡é¢„æµ‹è¾“å‡º
            binary_task_outputs: dict - äºŒå…ƒä»»åŠ¡é¢„æµ‹è¾“å‡º
            count_tasks: list - è®¡æ•°ä»»åŠ¡åç§°åˆ—è¡¨
            binary_tasks: list - äºŒå…ƒä»»åŠ¡åç§°åˆ—è¡¨
        """
        # ä¸»è¦SMILESæŸå¤±
        smiles_loss = self.smiles_loss_fn(smiles_output, smiles_target)
        
        # è¾…åŠ©ä»»åŠ¡æŸå¤±
        total_auxiliary_loss = 0.0
        num_aux_tasks = 0
        
        # è®¡æ•°ä»»åŠ¡æŸå¤± - é‡ç‚¹å…³æ³¨é‡è¦çš„åŒ–å­¦ç‰¹å¾
        count_loss = 0.0
        count_tasks_used = 0
        for idx, task in enumerate(count_tasks):
            if task in self.important_count_tasks:
                target = auxiliary_targets[:, idx].long()
                logits = count_task_outputs[task]
                loss = self.count_task_loss_fn(logits, target)
                count_loss += loss
                count_tasks_used += 1
        
        if count_tasks_used > 0:
            count_loss = count_loss / count_tasks_used  # å¹³å‡æŸå¤±
            total_auxiliary_loss += self.count_weight * count_loss
            num_aux_tasks += 1
        
        # äºŒå…ƒåˆ†ç±»ä»»åŠ¡æŸå¤± - ç»“æ„ç‰¹å¾
        binary_loss = 0.0
        binary_tasks_used = 0
        for idx, task in enumerate(binary_tasks):
            if task in self.important_binary_tasks:
                target = auxiliary_targets[:, len(count_tasks) + idx].float()
                logit = binary_task_outputs[task]
                loss = self.binary_task_loss_fn(logit, target)
                binary_loss += loss
                binary_tasks_used += 1
        
        if binary_tasks_used > 0:
            binary_loss = binary_loss / binary_tasks_used  # å¹³å‡æŸå¤±
            total_auxiliary_loss += self.binary_weight * binary_loss
            num_aux_tasks += 1
        
        # æ€»æŸå¤±
        if num_aux_tasks > 0:
            total_auxiliary_loss = total_auxiliary_loss / num_aux_tasks  # å¹³å‡è¾…åŠ©æŸå¤±
            total_loss = smiles_loss + self.aux_weight * total_auxiliary_loss
        else:
            total_loss = smiles_loss
            
        return total_loss, smiles_loss, total_auxiliary_loss


def get_adaptive_aux_weight(epoch, total_epochs, initial_weight=0.5, final_weight=0.1, decay_type='exponential'):
    """
    è‡ªé€‚åº”è°ƒæ•´è¾…åŠ©ä»»åŠ¡æƒé‡
    æ—©æœŸè®­ç»ƒæ—¶è¾…åŠ©ä»»åŠ¡æƒé‡è¾ƒå¤§ï¼Œå¸®åŠ©å­¦ä¹ è¯­ä¹‰ï¼›åæœŸæƒé‡å‡å°ï¼Œä¸“æ³¨SMILESç”Ÿæˆ
    """
    if decay_type == 'exponential':
        # æŒ‡æ•°è¡°å‡
        decay_rate = math.log(final_weight / initial_weight) / total_epochs
        weight = initial_weight * math.exp(decay_rate * epoch)
    elif decay_type == 'linear':
        # çº¿æ€§è¡°å‡
        weight = initial_weight - (initial_weight - final_weight) * epoch / total_epochs
    elif decay_type == 'cosine':
        # ä½™å¼¦è¡°å‡
        weight = final_weight + 0.5 * (initial_weight - final_weight) * (1 + math.cos(math.pi * epoch / total_epochs))
    else:
        weight = initial_weight
    
    return max(weight, final_weight)
from math import sqrt


def cosine_similarity(s1, s2):
    counter1, counter2 = Counter(s1), Counter(s2)
    intersection = set(counter1.keys()) & set(counter2.keys())
    numerator = sum([counter1[x] * counter2[x] for x in intersection])

    sum1 = sum([counter1[x] ** 2 for x in counter1.keys()])
    sum2 = sum([counter2[x] ** 2 for x in counter2.keys()])
    denominator = sqrt(sum1) * sqrt(sum2)

    return numerator / denominator if denominator != 0 else 0.0




def evaluate_at(epoch, model, dataloader, char2idx, idx2char, max_seq_length=100):
    model.eval()
    samples = []
    total_bleu_score = 0.0
    total_cos_score = 0.0
    total_correct = 0
    total_smiles = 0
    bad_sm_count = 0
    n_exact = 0
    maccs_sim, rdk_sim, morgan_sim, levs = [], [], [], []

    # Initialize auxiliary task metrics
    count_task_true = {task: [] for task in count_tasks}
    count_task_pred = {task: [] for task in count_tasks}
    binary_task_true = {task: [] for task in binary_tasks}
    binary_task_pred = {task: [] for task in binary_tasks}

    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    smoothie = SmoothingFunction().method4

    with torch.no_grad():
        for i, (ir, uv, c_spec, h_spec, high_mass, smiles_indices, auxiliary_targets, atom_types) in enumerate(tqdm(dataloader, desc="Evaluating", ncols=100)):
            # Move data to device
            ir = ir.to(device)
            uv = uv.to(device)
            c_spec = c_spec.to(device)
            h_spec = h_spec.to(device)
            high_mass = high_mass.to(device)
            smiles_indices = smiles_indices.to(device)
            auxiliary_targets = auxiliary_targets.to(device)
            atom_types = atom_types.to(device)
            batch_size = ir.size(0)

            # Get true SMILES
            true_smiles_list = []
            for j in range(batch_size):
                true_indices = smiles_indices[j]
                true_smiles_tokens = []
                for idx in true_indices:
                    idx = idx.item()
                    if idx == char2idx['<EOS>']:
                        break
                    elif idx not in [char2idx['<PAD>'], char2idx['<SOS>']]:
                        true_smiles_tokens.append(idx2char.get(idx, '<UNK>'))
                true_smiles_str = ''.join(true_smiles_tokens)
                true_smiles_list.append(true_smiles_str)


            # Greedy prediction
            predicted_smiles_list = predict_greedy(
                model, ir, uv, c_spec, h_spec, high_mass,
                char2idx, idx2char, max_seq_length=max_seq_length, atom_types=atom_types)



            for true_smiles_str, predicted_smiles_str in zip(true_smiles_list, predicted_smiles_list):
                # For validity
                try:
                    tmp = Chem.CanonSmiles(predicted_smiles_str)
                except:
                    bad_sm_count += 1

                try:
                    mol_output = Chem.MolFromSmiles(predicted_smiles_str)
                    mol_gt = Chem.MolFromSmiles(true_smiles_str)
                    if Chem.MolToInchi(mol_output) == Chem.MolToInchi(mol_gt):
                        n_exact += 1
                    maccs_sim.append(DataStructs.FingerprintSimilarity(MACCSkeys.GenMACCSKeys(mol_output), MACCSkeys.GenMACCSKeys(mol_gt), metric=DataStructs.TanimotoSimilarity))
                    rdk_sim.append(DataStructs.FingerprintSimilarity(Chem.RDKFingerprint(mol_output), Chem.RDKFingerprint(mol_gt), metric=DataStructs.TanimotoSimilarity))
                    morgan_sim.append(DataStructs.TanimotoSimilarity(AllChem.GetMorganFingerprint(mol_output, 2), AllChem.GetMorganFingerprint(mol_gt, 2)))
                except:
                    pass

                # Compute BLEU score
                reference = [list(true_smiles_str)]
                candidate = list(predicted_smiles_str)
                bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)
                total_bleu_score += bleu_score

                # Compute SMILES accuracy
                try:
                    tmp_1 = Chem.CanonSmiles(predicted_smiles_str)
                    tmp_2 = Chem.CanonSmiles(true_smiles_str)
                    if tmp_1 == tmp_2:
                        total_correct += 1
                except:
                    if predicted_smiles_str == true_smiles_str:
                        total_correct += 1
                total_smiles += 1

                # Compute Cos-sim
                cos_sim = cosine_similarity(predicted_smiles_str, true_smiles_str)
                total_cos_score += cos_sim

                # Compute L-Distance
                l_dis = lev(predicted_smiles_str, true_smiles_str)
                levs.append(l_dis)


            # Get auxiliary task predictions
            # Prepare dummy target sequence (only for getting auxiliary outputs)
            tgt_seq = torch.full((1, batch_size), char2idx['<SOS>'], dtype=torch.long, device=device)
            tgt_mask = model.smiles_decoder.generate_square_subsequent_mask(1).to(device)


            _, _, _, count_task_outputs, binary_task_outputs = model(
                ir, uv, c_spec, h_spec, high_mass, tgt_seq, tgt_mask, atom_types=atom_types)

            # Collect auxiliary task predictions and true values
            # Count tasks
            for idx_task, task in enumerate(count_tasks):
                target = auxiliary_targets[:, idx_task].cpu().numpy()  # Shape: [batch_size]
                logits = count_task_outputs[task]  # Shape: [batch_size, num_classes]
                predictions = logits.argmax(dim=1).cpu().numpy()  # Shape: [batch_size]
                count_task_true[task].extend(target)
                count_task_pred[task].extend(predictions)

            # Binary tasks
            for idx_task, task in enumerate(binary_tasks):
                target = auxiliary_targets[:, len(count_tasks) + idx_task].cpu().numpy()
                logit = binary_task_outputs[task]
                predictions = (torch.sigmoid(logit) > 0.5).int().cpu().numpy()
                binary_task_true[task].extend(target)
                binary_task_pred[task].extend(predictions)



        avg_bleu_score = total_bleu_score / total_smiles
        accuracy = total_correct / total_smiles
        # Compute validity
        validity  = (total_smiles - bad_sm_count) / total_smiles
        cos_sim_all = total_cos_score / total_smiles
        exact = n_exact * 1.0 / total_smiles


        results_dict = {
            'BLEU': avg_bleu_score,
            'validity': validity,
            'Levenshtein': np.mean(levs),
            'Cosine Similarity': cos_sim_all,
            'Top1 Acc': accuracy,
            'Exact': exact,
            "MACCS FTS": np.mean(maccs_sim),
            "RDKit FTS": np.mean(rdk_sim),
            "Morgan FTS": np.mean(morgan_sim),
        }

        # Compute auxiliary task metrics
        from sklearn.metrics import accuracy_score, mean_absolute_error

        count_task_acc = {}
        count_task_mae = {}
        for task in count_tasks:
            y_true = count_task_true[task]
            y_pred = count_task_pred[task]
            acc = accuracy_score(y_true, y_pred)
            mae = mean_absolute_error(y_true, y_pred)
            count_task_acc[task] = acc
            count_task_mae[task] = mae

        binary_task_acc = {}
        for task in binary_tasks:
            y_true = binary_task_true[task]
            y_pred = binary_task_pred[task]
            acc = accuracy_score(y_true, y_pred)
            binary_task_acc[task] = acc

        # # Display sample results
        # for i, sample in enumerate(samples):
        #     print(f"\nSample {i+1}:")
        #     print(f"True SMILES: {sample['True SMILES']}")
        #     print(f"Predicted SMILES: {sample['Predicted SMILES']}")
        #     print(f"BLEU Score: {sample['BLEU Score']:.4f}")
        #     print("Auxiliary Task Predictions:")
        #     for task in count_tasks + binary_tasks:
        #         true_value = sample['Auxiliary True'][task]
        #         predicted_value = sample['Auxiliary Predicted'][task]
        #         print(f"  {task}: True = {true_value}, Predicted = {predicted_value}")
        #     print("-" * 50)

        # Return metrics
        val_aux_metrics = {
            'count_task_acc': 0,
            'count_task_mae': 0,
            'binary_task_acc': 0,
        }


        # if epoch % 10 == 0:
        #     os.makedirs(f"./results_new/", exist_ok=True)
        #     with open(f"results_new/epoch_{epoch}_results.csv", 'w', newline='') as file:
        #         writer = csv.DictWriter(file, fieldnames=['pred_smiles', 'true_smiles', 'BLEU'])
        #         writer.writeheader()
        #         writer.writerows(results)

        return results_dict, val_aux_metrics
        # return avg_bleu_score, accuracy, val_aux_metrics



def evaluate(epoch, model, dataloader, char2idx, idx2char, max_seq_length=100):
    model.eval()
    samples = []
    total_bleu_score = 0.0
    total_cos_score = 0.0
    total_correct = 0
    total_smiles = 0
    bad_sm_count = 0
    n_exact = 0
    maccs_sim, rdk_sim, morgan_sim, levs = [], [], [], []

    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    smoothie = SmoothingFunction().method4

    with torch.no_grad():
        for i, (ir, uv, c_spec, h_spec, high_mass, smiles_indices, auxiliary_targets, atom_types) in enumerate(tqdm(dataloader, desc="Evaluating", ncols=100)):
            # Move data to device
            ir = ir.to(device)
            uv = uv.to(device)
            c_spec = c_spec.to(device)
            h_spec = h_spec.to(device)
            high_mass = high_mass.to(device)
            smiles_indices = smiles_indices.to(device)
            auxiliary_targets = auxiliary_targets.to(device)
            atom_types = atom_types.to(device)
            batch_size = ir.size(0)

            # Get true SMILES
            true_smiles_list = []
            for j in range(batch_size):
                true_indices = smiles_indices[j]
                true_smiles_tokens = []
                for idx in true_indices:
                    idx = idx.item()
                    if idx == char2idx['<EOS>']:
                        break
                    elif idx not in [char2idx['<PAD>'], char2idx['<SOS>']]:
                        true_smiles_tokens.append(idx2char.get(idx, '<UNK>'))
                true_smiles_str = ''.join(true_smiles_tokens)
                true_smiles_list.append(true_smiles_str)


            # Greedy prediction for training (faster and efficient)
            predicted_smiles_list = predict_greedy(
                model, ir, uv, c_spec, h_spec, high_mass,
                char2idx, idx2char, max_seq_length=max_seq_length, atom_types=atom_types)


            for true_smiles_str, predicted_smiles_str in zip(true_smiles_list, predicted_smiles_list):
                # For validity
                try:
                    tmp = Chem.CanonSmiles(predicted_smiles_str)
                except:
                    bad_sm_count += 1

                try:
                    mol_output = Chem.MolFromSmiles(predicted_smiles_str)
                    mol_gt = Chem.MolFromSmiles(true_smiles_str)
                    if Chem.MolToInchi(mol_output) == Chem.MolToInchi(mol_gt):
                        n_exact += 1
                    maccs_sim.append(DataStructs.FingerprintSimilarity(MACCSkeys.GenMACCSKeys(mol_output), MACCSkeys.GenMACCSKeys(mol_gt), metric=DataStructs.TanimotoSimilarity))
                    rdk_sim.append(DataStructs.FingerprintSimilarity(Chem.RDKFingerprint(mol_output), Chem.RDKFingerprint(mol_gt), metric=DataStructs.TanimotoSimilarity))
                    morgan_sim.append(DataStructs.TanimotoSimilarity(AllChem.GetMorganFingerprint(mol_output, 2), AllChem.GetMorganFingerprint(mol_gt, 2)))
                except:
                    pass

                # Compute BLEU score
                reference = [list(true_smiles_str)]
                candidate = list(predicted_smiles_str)
                bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)
                total_bleu_score += bleu_score

                # Compute SMILES accuracy
                if predicted_smiles_str == true_smiles_str:
                    total_correct += 1
                total_smiles += 1

                # Compute Cos-sim
                cos_sim = cosine_similarity(predicted_smiles_str, true_smiles_str)
                total_cos_score += cos_sim

                # Compute L-Distance
                l_dis = lev(predicted_smiles_str, true_smiles_str)
                levs.append(l_dis)



        avg_bleu_score = total_bleu_score / total_smiles
        accuracy = total_correct / total_smiles
        # Compute validity
        validity  = (total_smiles - bad_sm_count) / total_smiles
        cos_sim_all = total_cos_score / total_smiles
        exact = n_exact * 1.0 / total_smiles


        results_dict = {
            'BLEU': avg_bleu_score,
            'validity': validity,
            'Levenshtein': np.mean(levs),
            'Cosine Similarity': cos_sim_all,
            'Top1 Acc': accuracy,
            'Exact': exact,
            "MACCS FTS": np.mean(maccs_sim),
            "RDKit FTS": np.mean(rdk_sim),
            "Morgan FTS": np.mean(morgan_sim),
        }

        # if epoch % 10 == 0:
        #     os.makedirs(f"./results_new/", exist_ok=True)
        #     with open(f"results_new/epoch_{epoch}_results.csv", 'w', newline='') as file:
        #         writer = csv.DictWriter(file, fieldnames=['pred_smiles', 'true_smiles', 'BLEU'])
        #         writer.writeheader()
        #         writer.writerows(results)

        return results_dict, None



import torch.nn.functional as F
def predict_greedy(model, ir, uv, c_spec, h_spectrum, high_mass, char2idx, idx2char, max_seq_length=100, atom_types=None):
    model.eval()
    with torch.no_grad():
        # Split h_spectrum into h_spectrum_part, f_spectrum, n_spectrum
        h_spectrum_part = h_spectrum[:, :382]
        f_spectrum = h_spectrum[:, 382:394]
        n_spectrum = h_spectrum[:, 394:408]
        o_spectrum = h_spectrum[:, 408:]

        # Prepare features
        features = {
            'ir': ir,
            'uv': uv,
            'nmr_c': c_spec,
            'nmr_h': h_spectrum_part,
            'f_spectrum': f_spectrum,
            'n_spectrum': n_spectrum,
            'o_spectrum': o_spectrum,
            'mass_high': high_mass
        }

        # Tokenize features
        tokens = model.tokenizer(features)  # Shape: [batch_size, total_N_features, d_model]

        # Permute for transformer input: [seq_len, batch_size, d_model]
        tokens = tokens.permute(1, 0, 2)

        # Apply transformer encoder
        memory, attention = model.transformer_encoder(tokens)  # Shape: [seq_len, batch_size, d_model]

        batch_size = ir.size(0)
        device = ir.device

        # Initialize input sequence with <SOS> tokens
        tgt_indices = torch.full((1, batch_size), char2idx['<SOS>'], dtype=torch.long, device=device)  # Shape: [1, batch_size]

        generated_tokens = []

        for _ in range(max_seq_length):
            # Generate target mask
            tgt_mask = model.smiles_decoder.generate_square_subsequent_mask(tgt_indices.size(0)).to(device)

            # Decode using the SMILES decoder
            output = model.smiles_decoder(
                tgt_indices,
                memory,
                tgt_mask=tgt_mask,
                atom_types=atom_types
            )  # Shape: [tgt_seq_len, batch_size, vocab_size]

            # Get the last timestep output
            output_logits = output[-1, :, :]  # Shape: [batch_size, vocab_size]

            # Greedy decoding: select the token with highest probability
            next_token = output_logits.argmax(dim=-1)  # Shape: [batch_size]

            generated_tokens.append(next_token.unsqueeze(0))  # Shape: [1, batch_size]

            # Append the next token to the target indices
            tgt_indices = torch.cat([tgt_indices, next_token.unsqueeze(0)], dim=0)  # Shape: [tgt_seq_len + 1, batch_size]

            # Stop if all sequences have generated <EOS>
            if (next_token == char2idx['<EOS>']).all():
                break

        # Convert generated token indices to SMILES strings
        generated_tokens = torch.cat(generated_tokens, dim=0)  # Shape: [generated_seq_len, batch_size]
        generated_smiles = []
        for i in range(batch_size):
            token_indices = generated_tokens[:, i].cpu().numpy()
            tokens = []
            for idx in token_indices:
                if idx == char2idx['<EOS>']:
                    break
                elif idx not in [char2idx['<PAD>'], char2idx['<SOS>']]:
                    tokens.append(idx2char.get(idx, '<UNK>'))
            smiles_str = ''.join(tokens)
            generated_smiles.append(smiles_str)

        return generated_smiles





# å®šä¹‰æƒé‡è°ƒèŠ‚å‡½æ•°
def get_smiles_weight(epoch, total_epochs, k=5):
    """
    è®¡ç®— SMILES æŸå¤±çš„æƒé‡ï¼ŒåŸºäºæŒ‡æ•°å¢é•¿ã€‚
    Args:
        epoch (int): å½“å‰çš„è®­ç»ƒè½®æ¬¡
        total_epochs (int): æ€»è®­ç»ƒè½®æ¬¡
        k (float): æ§åˆ¶å¢é•¿é€Ÿç‡çš„å‚æ•°
    Returns:
        float: SMILES æŸå¤±çš„æƒé‡
    """
    return 1 - math.exp(-k * epoch / total_epochs)




def train_at(model, smiles_loss_fn, optimizer, train_dataloader, val_dataloader, epochs=10, save_dir='./model_weights_smiles'):
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    best_bleu_score = 0.0  # ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹
    feature_loss_fn = nn.MSELoss()

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch+1}/{epochs}]", total=len(train_dataloader), ncols=100)
        for i, (ir, uv, c_spec, h_spec, high_mass, smiles_indices, auxiliary_targets, atom_types) in enumerate(progress_bar):
            # Move data to device
            ir = ir.to(device)
            uv = uv.to(device)
            c_spec = c_spec.to(device)
            h_spec = h_spec.to(device)
            high_mass = high_mass.to(device)
            smiles_indices = smiles_indices.to(device)
            auxiliary_targets = auxiliary_targets.to(device)
            atom_types = atom_types.to(device)

            optimizer.zero_grad()

            # Prepare target sequence
            tgt_seq = smiles_indices.transpose(0, 1)[:-1]
            tgt_output = smiles_indices.transpose(0, 1)[1:]

            # Generate mask
            seq_len = tgt_seq.size(0)
            tgt_mask = model.smiles_decoder.generate_square_subsequent_mask(seq_len).to(device)

            # Forward pass
            output_spectra, output_mol, spectra_feat, count_task_outputs, binary_task_outputs = model(
                ir, uv, c_spec, h_spec, high_mass, tgt_seq, tgt_mask, atom_types)

            # Compute main task loss (from spectral features)
            output_flat = output_spectra.reshape(-1, output_spectra.size(-1))
            tgt_output_flat = tgt_output.reshape(-1)
            loss_smiles_spectra = smiles_loss_fn(output_flat, tgt_output_flat)

            # # Compute loss from molecular features
            # output_mol_flat = output_mol.reshape(-1, output_mol.size(-1))
            # loss_smiles_mol = smiles_loss_fn(output_mol_flat, tgt_output_flat)

            # # Compute feature loss between spectra_feat and mol_feat
            # feature_loss = feature_loss_fn(spectra_feat, mol_feat)

            # # Total loss
            # total_loss = loss_smiles_spectra

            # è®¡ç®—è¾…åŠ©ä»»åŠ¡æŸå¤±
            total_auxiliary_loss = 0.0

            # è®¡æ•°ä»»åŠ¡
            for idx, task in enumerate(count_tasks):
                target = auxiliary_targets[:, idx].long()
                logits = count_task_outputs[task]
                loss = count_task_loss_fn(logits, target)
                total_auxiliary_loss += loss

            # äºŒå…ƒåˆ†ç±»ä»»åŠ¡
            for idx, task in enumerate(binary_tasks):
                target = auxiliary_targets[:, len(count_tasks) + idx].float()
                logit = binary_task_outputs[task]
                loss = binary_task_loss_fn(logit, target)
                total_auxiliary_loss += loss

            # æ€»æŸå¤±
            total_loss = loss_smiles_spectra + 0.1*total_auxiliary_loss
            # total_loss = total_auxiliary_loss
            # if epoch < 50:
            #     total_loss = total_auxiliary_loss
            # else:
            #     total_loss = loss_smiles_spectra
            # # åŠ¨æ€è°ƒæ•´æ€»æŸå¤±
            # w_smiles = get_smiles_weight(epoch, 200, k=5)
            # total_loss = w_smiles * loss_smiles_spectra + (1 - w_smiles) * total_auxiliary_loss

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            total_loss.backward()
            optimizer.step()

            # æ—¥å¿—è®°å½•
            avg_loss = total_loss.item() / (i + 1)
            progress_bar.set_postfix({'Loss': avg_loss})

        # æ¯ä¸ª epoch ç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        print(f"\nEpoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}")
        # print(f"\nEpoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}, Weight for the main task: {w_smiles:.4f}")
        print(f"Epoch: {epoch+1}, starting validation...")

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_metrics, val_aux_metrics = evaluate(
            epoch, model, val_dataloader, char2idx, idx2char, max_seq_length=max_seq_length)
        # val_bleu_score, val_acc = evaluate(
        #     model, val_dataloader, char2idx, idx2char, max_seq_length=max_seq_length)
        for key, value in val_metrics.items():
            print(f"{key}: {value:.4f}")

        # æ‰“å°è¾…åŠ©ä»»åŠ¡æŒ‡æ ‡
        print("\nValidation Auxiliary Task Metrics:")
        for task in count_tasks:
            acc = val_aux_metrics['count_task_acc'][task]
            mae = val_aux_metrics['count_task_mae'][task]
            if acc < 0.9:
                print(f"Count Task - {task}: Accuracy = {acc:.4f}, MAE = {mae:.4f}")
        
        for task in binary_tasks:
            acc = val_aux_metrics['binary_task_acc'][task]
            if acc < 0.9:
                print(f"Binary Task - {task}: Accuracy = {acc:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['BLEU'] > best_bleu_score:
            best_bleu_score = val_metrics['BLEU']
            torch.save(model.state_dict(), os.path.join(save_dir, 'ir_only_scaffold.pth'))
            print(f"Best model saved with BLEU Score: {best_bleu_score:.4f}")




def train_with_semantic_supervision(model, smiles_loss_fn, optimizer, train_dataloader, val_dataloader, 
                                   epochs=10, save_dir='./model_weights_smiles', use_adaptive_weight=True):
    """
    ä½¿ç”¨è¯­ä¹‰ç›‘ç£çš„è®­ç»ƒå‡½æ•°ï¼ŒåŒ…å«è¾…åŠ©ä»»åŠ¡æŸå¤±
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    best_bleu_score = 0.0
    
    # ä½¿ç”¨è¯­ä¹‰ç›‘ç£æŸå¤±å‡½æ•°
    if isinstance(smiles_loss_fn, SemanticSupervisedSMILESLoss):
        semantic_loss_fn = smiles_loss_fn
    else:
        # å¦‚æœä¼ å…¥çš„æ˜¯æ™®é€šæŸå¤±å‡½æ•°ï¼Œåˆ›å»ºè¯­ä¹‰ç›‘ç£ç‰ˆæœ¬
        ignore_index = char2idx['<PAD>']
        semantic_loss_fn = SemanticSupervisedSMILESLoss(ignore_index=ignore_index, aux_weight=0.1)

    for epoch in range(epochs):
        model.train()
        total_loss_sum = 0
        smiles_loss_sum = 0
        aux_loss_sum = 0
        
        # è‡ªé€‚åº”è°ƒæ•´è¾…åŠ©ä»»åŠ¡æƒé‡
        if use_adaptive_weight:
            current_aux_weight = get_adaptive_aux_weight(epoch, epochs, initial_weight=0.5, final_weight=0.1)
            semantic_loss_fn.aux_weight = current_aux_weight
        
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch+1}/{epochs}]", total=len(train_dataloader), ncols=100)
        
        for i, (ir, uv, c_spec, h_spec, high_mass, smiles_indices, auxiliary_targets, atom_types) in enumerate(progress_bar):
            # Move data to device
            ir = ir.to(device)
            uv = uv.to(device)
            c_spec = c_spec.to(device)
            h_spec = h_spec.to(device)
            high_mass = high_mass.to(device)
            smiles_indices = smiles_indices.to(device)
            auxiliary_targets = auxiliary_targets.to(device)
            atom_types = atom_types.to(device)

            optimizer.zero_grad()

            # Prepare target sequence
            tgt_seq = smiles_indices.transpose(0, 1)[:-1]
            tgt_output = smiles_indices.transpose(0, 1)[1:]

            # Generate mask
            seq_len = tgt_seq.size(0)
            tgt_mask = model.smiles_decoder.generate_square_subsequent_mask(seq_len).to(device)

            # Forward pass
            output_spectra, attention, fusion_feat, count_task_outputs, binary_task_outputs = model(
                ir, uv, c_spec, h_spec, high_mass, tgt_seq, tgt_mask, atom_types)

            # Prepare inputs for semantic loss
            output_flat = output_spectra.reshape(-1, output_spectra.size(-1))
            tgt_output_flat = tgt_output.reshape(-1)

            # è®¡ç®—è¯­ä¹‰ç›‘ç£æŸå¤±
            total_loss, smiles_loss, aux_loss = semantic_loss_fn(
                output_flat, tgt_output_flat, auxiliary_targets, 
                count_task_outputs, binary_task_outputs, count_tasks, binary_tasks
            )

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            total_loss.backward()
            optimizer.step()

            # ç´¯ç§¯æŸå¤±ç”¨äºæ—¥å¿—
            total_loss_sum += total_loss.item()
            smiles_loss_sum += smiles_loss.item()
            aux_loss_sum += aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_total_loss = total_loss_sum / (i + 1)
            avg_smiles_loss = smiles_loss_sum / (i + 1)
            avg_aux_loss = aux_loss_sum / (i + 1)
            
            progress_bar.set_postfix({
                'Total': f'{avg_total_loss:.4f}',
                'SMILES': f'{avg_smiles_loss:.4f}',
                'Aux': f'{avg_aux_loss:.4f}',
                'Aux_W': f'{semantic_loss_fn.aux_weight:.3f}'
            })

        # æ¯ä¸ª epoch ç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        print(f"\nEpoch [{epoch+1}/{epochs}] Training Summary:")
        print(f"  Total Loss: {avg_total_loss:.4f}")
        print(f"  SMILES Loss: {avg_smiles_loss:.4f}")
        print(f"  Auxiliary Loss: {avg_aux_loss:.4f}")
        print(f"  Auxiliary Weight: {semantic_loss_fn.aux_weight:.3f}")
        print(f"Starting validation...")

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_metrics, val_aux_metrics = evaluate(
            epoch, model, val_dataloader, char2idx, idx2char, max_seq_length=max_seq_length)
        
        # æ‰“å°ä¸»è¦æŒ‡æ ‡
        for key, value in val_metrics.items():
            print(f"{key}: {value:.4f}")

        # Save best model
        val_bleu_score = val_metrics['BLEU']
        if val_bleu_score > best_bleu_score:
            best_bleu_score = val_bleu_score
            print(f"ğŸ‰ New best model with BLEU Score: {best_bleu_score:.4f}")
            torch.save(model.state_dict(), os.path.join(save_dir, 'best_semantic_supervised.pth'))


def train(model, smiles_loss_fn, optimizer, train_dataloader, val_dataloader, epochs=10, save_dir='./model_weights_smiles'):
    """
    ä¿æŒåŸæœ‰çš„è®­ç»ƒå‡½æ•°ä½œä¸ºå‘åå…¼å®¹
    """
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    best_bleu_score = 0.0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch [{epoch+1}/{epochs}]", total=len(train_dataloader), ncols=100)
        for i, (ir, uv, c_spec, h_spec, high_mass, smiles_indices, auxiliary_targets, atom_types) in enumerate(progress_bar):
            # Move data to device
            ir = ir.to(device)
            uv = uv.to(device)
            c_spec = c_spec.to(device)
            h_spec = h_spec.to(device)
            high_mass = high_mass.to(device)
            smiles_indices = smiles_indices.to(device)
            auxiliary_targets = auxiliary_targets.to(device)
            atom_types = atom_types.to(device)

            optimizer.zero_grad()

            # Prepare target sequence
            tgt_seq = smiles_indices.transpose(0, 1)[:-1]
            tgt_output = smiles_indices.transpose(0, 1)[1:]

            # Generate mask
            seq_len = tgt_seq.size(0)
            tgt_mask = model.smiles_decoder.generate_square_subsequent_mask(seq_len).to(device)

            # Forward pass
            output_spectra, attention, fusion_feat, count_task_outputs, binary_task_outputs = model(
                ir, uv, c_spec, h_spec, high_mass, tgt_seq, tgt_mask, atom_types)

            # Compute main task loss (from spectral features)
            output_flat = output_spectra.reshape(-1, output_spectra.size(-1))
            tgt_output_flat = tgt_output.reshape(-1)
            loss_smiles_spectra = smiles_loss_fn(output_flat, tgt_output_flat)

            # æ€»æŸå¤±
            total_loss = loss_smiles_spectra

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            total_loss.backward()
            optimizer.step()

            # æ—¥å¿—è®°å½•
            avg_loss = total_loss.item() / (i + 1)
            progress_bar.set_postfix({'Loss': avg_loss})

        # æ¯ä¸ª epoch ç»“æŸååœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        print(f"\nEpoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}")
        print(f"Epoch: {epoch+1}, starting validation...")

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_metrics, val_aux_metrics = evaluate(
            epoch, model, val_dataloader, char2idx, idx2char, max_seq_length=max_seq_length)
        for key, value in val_metrics.items():
            print(f"{key}: {value:.4f}")

        # Save best model
        val_bleu_score = val_metrics['BLEU']
        if val_bleu_score > best_bleu_score:
            best_bleu_score = val_bleu_score
            print(f"Current best model with BLEU Score: {best_bleu_score:.4f}")
            torch.save(model.state_dict(), os.path.join(save_dir, 'tmp.pth'))





import torch.nn.functional as F

SMILES_VOCAB = ['<PAD>', '<SOS>', '<EOS>', '<UNK>',
                'C', 'N', 'O', 'F',
                '1', '2', '3', '4', '5',
                '#', '=', '(', ')',
                ]
vocab_size = len(SMILES_VOCAB)

# åˆ›å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„å’Œç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„
char2idx = {token: idx for idx, token in enumerate(SMILES_VOCAB)}
idx2char = {idx: token for idx, token in enumerate(SMILES_VOCAB)}




# uv
print('load uv file...')
uv_max_value = 15.0
uv_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/uv.csv')
peak_columns = [col for col in uv_spe_filtered.columns if 'peak' in col]
uv_spe_filtered[peak_columns] = uv_spe_filtered[peak_columns] / uv_max_value
uv_spe_filtered = uv_spe_filtered.to_numpy()
print('uv_spe_filtered:', uv_spe_filtered.shape)


# ir
print('load ir file...')
ir_max_value = 4000.0
ir_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/ir_82.csv')
peak_columns = [col for col in ir_spe_filtered.columns if 'peak' in col]
ir_spe_filtered[peak_columns] = ir_spe_filtered[peak_columns] / ir_max_value
ir_spe_filtered = ir_spe_filtered.to_numpy()
print('ir_spe_filtered:', ir_spe_filtered.shape)


# c-nmr
print('load 1dc-nmr with dept file...')
cnmr_max_value = 220.0
cnmr_min_value = -10.0
nmrc_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/1d_cnmr_dept.csv')
peak_columns = [col for col in nmrc_spe_filtered.columns if 'peak' in col]
nmrc_spe_filtered[peak_columns] = (nmrc_spe_filtered[peak_columns] - cnmr_min_value) / (cnmr_max_value - cnmr_min_value)
nmrc_spe_filtered = nmrc_spe_filtered.to_numpy()

print('load 2dc-nmr (c-c, c-x) file...')
cnmr_2d_max_value = 450.0
cnmr_2d_min_value = -400.0
twoD_nmr = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/2d_cnmr_ina_chsqc.csv')
peak_columns = [col for col in twoD_nmr.columns if 'peak' in col]
twoD_nmr[peak_columns] = (twoD_nmr[peak_columns] - cnmr_2d_min_value) / (cnmr_2d_max_value - cnmr_2d_min_value)
twoD_nmr = twoD_nmr.to_numpy()
nmrc_spe_filtered = np.concatenate((nmrc_spe_filtered, twoD_nmr), axis=1)
print('nmrc_spe_filtered:', nmrc_spe_filtered.shape)



# h-nmr
print('load 1d h-nmr file...')
nmrh_max_value = 12.0
nmrh_min_value = -2.0
nmrh_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/1d_hnmr.csv')
peak_columns = [col for col in nmrh_spe_filtered.columns if 'peak' in col]

# è¿‡æ»¤H-NMRå¼‚å¸¸å€¼ - å…ˆè¯†åˆ«å¼‚å¸¸æ ·æœ¬ï¼Œä½†ä¸å¯¹å…¶å½’ä¸€åŒ–
print('Filtering H-NMR samples with abnormal values...')
threshold = 500.0
nmrh_max_values = nmrh_spe_filtered[peak_columns].max(axis=1)
h_nmr_abnormal_mask = nmrh_max_values > threshold
h_nmr_abnormal_indices = set(np.where(h_nmr_abnormal_mask)[0])

# ä½¿ç”¨ä½ è®¾å®šçš„minå’Œmaxå€¼è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå¯¹æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬å¼‚å¸¸æ ·æœ¬ï¼Œä½†å¼‚å¸¸æ ·æœ¬ç¨åä¼šè¢«è¿‡æ»¤æ‰ï¼‰
nmrh_spe_filtered[peak_columns] = (nmrh_spe_filtered[peak_columns] - nmrh_min_value) / (nmrh_max_value - nmrh_min_value)
nmrh_spe_filtered = nmrh_spe_filtered.to_numpy()


# HSQC
hsqc_max_value = 400.0
hsqc_min_value = -350.0
hsqc = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/2d_hhsqc.csv')
peak_columns = [col for col in hsqc.columns if 'peak' in col]
hsqc[peak_columns] = (hsqc[peak_columns] - hsqc_min_value) / (hsqc_max_value - hsqc_min_value)
hsqc = hsqc.to_numpy()


# COSY
cosy_max_value = 14.0
cosy_min_value = -2.0
nmr_cosy = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/2d_hcosy.csv')
hxyh_columns = [col for col in nmr_cosy.columns if 'H_X_Y_H' in col]
nmr_cosy = nmr_cosy[hxyh_columns]
peak_columns = [col for col in nmr_cosy.columns if 'peak' in col]
nmr_cosy[peak_columns] = (nmr_cosy[peak_columns] - cosy_min_value) / (cosy_max_value - cosy_min_value)
nmr_cosy = nmr_cosy.to_numpy()

# # J2D
# j2d_max_value = 30.0
# j2d_min_value = -30.0
# j2d = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/2d_h_j2d.csv')
# j2d_columns = [col for col in j2d.columns if 'coupling' in col]
# j2d = j2d[j2d_columns]

# # è¿‡æ»¤J2Då¼‚å¸¸å€¼ - æ­£ç¡®æ£€æµ‹å¼‚å¸¸å€¼
# print('Filtering J2D samples with abnormal values...')
# j2d_max_values = j2d[j2d_columns].abs().max(axis=1)
# j2d_abnormal_mask = j2d_max_values > threshold
# j2d_abnormal_indices = set(np.where(j2d_abnormal_mask)[0])

# # ä½¿ç”¨è®¾å®šçš„minå’Œmaxå€¼è¿›è¡Œå½’ä¸€åŒ–
# j2d[j2d_columns] = (j2d[j2d_columns] - j2d_min_value) / (j2d_max_value - j2d_min_value)
# j2d = j2d.to_numpy()

# åˆå¹¶æ‰€æœ‰å¼‚å¸¸æ ·æœ¬ç´¢å¼•
# all_abnormal_indices = h_nmr_abnormal_indices.union(j2d_abnormal_indices)
all_abnormal_indices = h_nmr_abnormal_indices
all_abnormal_indices = sorted(list(all_abnormal_indices))
print(f"Found {len(all_abnormal_indices)} samples with abnormal values (> {threshold})")

if len(all_abnormal_indices) > 0:
    print(f"First 10 abnormal sample indices: {all_abnormal_indices[:10]}")


# x-nmr
print('load x-nmr file...')
# F-NMR
fnmr_max_value = 0.0001
fnmr_min_value = -400.0
nmrf_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/1d_fnmr.csv')
peak_columns = [col for col in nmrf_spe_filtered.columns if 'peak' in col]
nmrf_spe_filtered[peak_columns] = (nmrf_spe_filtered[peak_columns] - fnmr_min_value) / (fnmr_max_value - fnmr_min_value)
nmrf_spe_filtered = nmrf_spe_filtered.to_numpy()

# N-NMR  
nnmr_max_value = 400.0
nnmr_min_value = -260.0
nmrn_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/1d_nnmr.csv')
peak_columns = [col for col in nmrn_spe_filtered.columns if 'peak' in col]
nmrn_spe_filtered[peak_columns] = (nmrn_spe_filtered[peak_columns] - nnmr_min_value) / (nnmr_max_value - nnmr_min_value)
nmrn_spe_filtered = nmrn_spe_filtered.to_numpy()

# O-NMR
onmr_max_value = 460.0
onmr_min_value = -385.0
nmro_spe_filtered = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/1d_onmr.csv')
peak_columns = [col for col in nmro_spe_filtered.columns if 'peak' in col]
nmro_spe_filtered[peak_columns] = (nmro_spe_filtered[peak_columns] - onmr_min_value) / (onmr_max_value - onmr_min_value)
nmro_spe_filtered = nmro_spe_filtered.to_numpy()

# combine all h-nmr and x-nmr features together
nmrh_spe_filtered = np.concatenate((nmrh_spe_filtered, hsqc, nmr_cosy, nmrf_spe_filtered, nmrn_spe_filtered, nmro_spe_filtered), axis=1)
# nmrh_spe_filtered = np.concatenate((nmrh_spe_filtered, hsqc, nmr_cosy, j2d, nmrf_spe_filtered, nmrn_spe_filtered, nmro_spe_filtered), axis=1)

print('nmrh_spe_filtered:', nmrh_spe_filtered.shape)


# zhipu
print('load high-mass file...')
mass = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/ms.csv')
high_mass_spe = mass.to_numpy()
print('high-mass_spe:', high_mass_spe.shape)


# atom type
atom_type = high_mass_spe[:, 1:-1]
print(f"Atom type shape: {atom_type.shape}")


# smiles
smiles_list = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/qm9_all_raw_spe/smiles.csv').values.tolist() ### [[smiles1], [smiles2], ...]
smiles_lengths = [len(smiles[0]) for smiles in smiles_list]
max_smiles_length = max(smiles_lengths)
max_seq_length = max_smiles_length + 2
print(f"SMILES åºåˆ—çš„æœ€å¤§é•¿åº¦ä¸ºï¼š{max_smiles_length}")
print(f"æ¨¡å‹ä¸­åº”ä½¿ç”¨çš„ max_seq_length ä¸ºï¼š{max_seq_length}")


# è·å–æ‰€æœ‰è¾…åŠ©ä»»åŠ¡
# # Get the list of columns
# # auxiliary_data = pd.read_csv('/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/aligned_smiles_id_aux_task_canonical.csv')
# auxiliary_data = pd.read_csv('/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/aligned_smiles_id_aux_task.csv')
# columns = auxiliary_data.columns.tolist()
# # Exclude 'smiles' and 'id' columns to get auxiliary tasks
# auxiliary_tasks = [col for col in columns if col not in ['smiles', 'id']]
# print(f"Auxiliary tasks: {auxiliary_tasks}")



# file_prefixes = {
#     "c_nmr": '/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/Auxiliary_Task/C_NMR_TA.csv',
#     "h_nmr": '/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/Auxiliary_Task/H_NMR_TA.csv',
#     # "ir": '/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/Auxiliary_Task/IR_TA.csv',
#     "ms": '/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/smiles-transformer-master/Auxiliary_Task/MS_TA.csv',
# }
# auxiliary_data = pd.DataFrame()
# for prefix, filepath in file_prefixes.items():
#     df = pd.read_csv(filepath).iloc[:, 3:]
#     df.columns = [f"{prefix}_{col}" for col in df.columns]
#     auxiliary_data = pd.concat([auxiliary_data, df], axis=1)


auxiliary_data = pd.read_csv('/data4/linkaiqing/sm_pretrained/gp/aligned_smiles_id_aux_task.csv').iloc[:, 2:]


columns = auxiliary_data.columns.tolist()
auxiliary_tasks = [col for col in columns]
# auxiliary_tasks = ['ring_count']

# ä» auxiliary_data ä¸­ç­›é€‰åŒ…å« "ring" çš„åˆ—
# ring_columns = [col for col in auxiliary_data.columns if "ring" in col.lower()]
# ring_columns = [
#     "c_nmr_Ring_size1", "c_nmr_Ring_size2", "c_nmr_Ring_size3", "c_nmr_Ring_size4", "c_nmr_Ring_size5", "c_nmr_Ring_size6",
#     "h_nmr_H_connected_ring_size1", "h_nmr_H_connected_ring_size2", "h_nmr_H_connected_ring_size3", "h_nmr_H_connected_ring_size4", "h_nmr_H_connected_ring_size5", "h_nmr_H_connected_ring_size6", "h_nmr_H_connected_ring_size7", "h_nmr_H_connected_ring_size8",
# ]
# # åªä¿ç•™å¸¦æœ‰ "ring" çš„ç‰¹å¾
# auxiliary_data = auxiliary_data[ring_columns]
# # æ›´æ–° auxiliary_tasks åˆ—è¡¨
# auxiliary_tasks = ring_columns
print(f"Auxiliary tasks: {auxiliary_tasks}")
print(f"Number of ATs: {len(auxiliary_tasks)}")




def get_indices(smiles_series, smiles_to_index):
    indices = []
    missing_smiles = []
    for smiles in smiles_series:
        idx = smiles_to_index.get(smiles)
        if idx is not None:
            indices.append(idx)
        else:
            missing_smiles.append(smiles)
    return indices, missing_smiles


    

# å…ˆè¯»å–æ•°æ®é›†åˆ’åˆ†æ–‡ä»¶
train_df = pd.read_csv(f'/data4/linkaiqing/sm_pretrained/gp/csv/dataset/{data_split_mode}/train.csv')
val_df = pd.read_csv(f'/data4/linkaiqing/sm_pretrained/gp/csv/dataset/{data_split_mode}/val.csv')
test_df = pd.read_csv(f'/data4/linkaiqing/sm_pretrained/gp/csv/dataset/{data_split_mode}/test.csv')

# å¦‚æœæœ‰å¼‚å¸¸æ ·æœ¬ï¼Œå…ˆä»æ•°æ®åˆ’åˆ†æ–‡ä»¶ä¸­ç§»é™¤å¯¹åº”çš„SMILES
if len(all_abnormal_indices) > 0:
    print(f"Processing {len(all_abnormal_indices)} abnormal samples...")
    
    # è·å–å¼‚å¸¸æ ·æœ¬å¯¹åº”çš„SMILES
    abnormal_smiles = set()
    for idx in all_abnormal_indices:
        if idx < len(smiles_list):
            abnormal_smiles.add(smiles_list[idx][0])
    
    print(f"Found {len(abnormal_smiles)} unique abnormal SMILES")
    
    # ä»å„ä¸ªæ•°æ®é›†ä¸­ç§»é™¤å¼‚å¸¸SMILES
    original_train_size = len(train_df)
    original_val_size = len(val_df)
    original_test_size = len(test_df)
    
    train_df = train_df[~train_df['smiles'].isin(abnormal_smiles)].reset_index(drop=True)
    val_df = val_df[~val_df['smiles'].isin(abnormal_smiles)].reset_index(drop=True)
    test_df = test_df[~test_df['smiles'].isin(abnormal_smiles)].reset_index(drop=True)
    
    print(f"Train set: {original_train_size} -> {len(train_df)} (removed {original_train_size - len(train_df)})")
    print(f"Val set: {original_val_size} -> {len(val_df)} (removed {original_val_size - len(val_df)})")
    print(f"Test set: {original_test_size} -> {len(test_df)} (removed {original_test_size - len(test_df)})")
    
    # ç„¶åè¿‡æ»¤åŸå§‹æ•°æ®
    total_samples = len(smiles_list)
    normal_mask = np.ones(total_samples, dtype=bool)
    normal_mask[all_abnormal_indices] = False
    
    # è¿‡æ»¤æ‰€æœ‰æ•°ç»„
    ir_spe_filtered = ir_spe_filtered[normal_mask]
    uv_spe_filtered = uv_spe_filtered[normal_mask]
    nmrh_spe_filtered = nmrh_spe_filtered[normal_mask]
    nmrc_spe_filtered = nmrc_spe_filtered[normal_mask]
    high_mass_spe = high_mass_spe[normal_mask]
    atom_type = atom_type[normal_mask]
    
    # ä½¿ç”¨åŸå§‹çš„smiles_liståˆ›å»ºè¿‡æ»¤åçš„åˆ—è¡¨
    original_smiles_list = smiles_list.copy()
    smiles_list = [original_smiles_list[i] for i in range(total_samples) if normal_mask[i]]
    auxiliary_data = auxiliary_data[normal_mask].reset_index(drop=True)
    
    print(f"Filtered dataset: {total_samples} -> {len(smiles_list)} samples")

print(f"Final dataset size: {len(smiles_list)}")
print(f"ç¡®ä¿æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥...")
assert len(smiles_list) == len(auxiliary_data) == ir_spe_filtered.shape[0], "Data length mismatch after filtering!"

# åˆ›å»º SMILES åˆ°ç´¢å¼•çš„æ˜ å°„
smiles_to_index = {smiles[0]: idx for idx, smiles in enumerate(smiles_list)}

# 4. è·å–å„æ•°æ®é›†çš„ç´¢å¼•
train_indices, train_missing_smiles = get_indices(train_df['smiles'], smiles_to_index)
val_indices, val_missing_smiles = get_indices(val_df['smiles'], smiles_to_index)
test_indices, test_missing_smiles = get_indices(test_df['smiles'], smiles_to_index)


# åˆ’åˆ†è®­ç»ƒé›†æ•°æ®
train_ir_spe_filtered = ir_spe_filtered[train_indices]
train_uv_spe_filtered = uv_spe_filtered[train_indices]
train_nmrh_spe_filtered = nmrh_spe_filtered[train_indices]
train_nmrc_spe_filtered = nmrc_spe_filtered[train_indices]
train_high_mass_spe = high_mass_spe[train_indices]
train_smiles_list = [smiles_list[idx] for idx in train_indices]
train_aux_data = auxiliary_data.iloc[train_indices].reset_index(drop=True)
atom_types_list_train = atom_type[train_indices]



# åˆ’åˆ†éªŒè¯é›†æ•°æ®
val_ir_spe_filtered = ir_spe_filtered[val_indices]
val_uv_spe_filtered = uv_spe_filtered[val_indices]
val_nmrh_spe_filtered = nmrh_spe_filtered[val_indices]
val_nmrc_spe_filtered = nmrc_spe_filtered[val_indices]
val_high_mass_spe = high_mass_spe[val_indices]
val_smiles_list = [smiles_list[idx] for idx in val_indices]
val_aux_data = auxiliary_data.iloc[val_indices].reset_index(drop=True)
atom_types_list_val = atom_type[val_indices]

# åˆ’åˆ†æµ‹è¯•é›†æ•°æ®
test_ir_spe_filtered = ir_spe_filtered[test_indices]
test_uv_spe_filtered = uv_spe_filtered[test_indices]
test_nmrh_spe_filtered = nmrh_spe_filtered[test_indices]
test_nmrc_spe_filtered = nmrc_spe_filtered[test_indices]
test_high_mass_spe = high_mass_spe[test_indices]
test_smiles_list = [smiles_list[idx] for idx in test_indices]
test_aux_data = auxiliary_data.iloc[test_indices].reset_index(drop=True)
atom_types_list_test = atom_type[test_indices]



# å®šä¹‰ count_tasks å’Œ binary_tasks
count_tasks = [at for at in auxiliary_tasks if 'Has' not in at and 'Is' not in at]
binary_tasks = [at for at in auxiliary_tasks if 'Has' in at or 'Is' in at]


# åˆ›å»ºè®­ç»ƒé›†æ•°æ®é›†
train_dataset = SpectraDataset(
    ir_spectra=train_ir_spe_filtered,
    uv_spectra=train_uv_spe_filtered,
    c_spectra=train_nmrc_spe_filtered,
    h_spectra=train_nmrh_spe_filtered,
    high_mass_spectra=train_high_mass_spe,
    smiles_list=train_smiles_list,
    auxiliary_data=train_aux_data,
    char2idx=char2idx,
    max_seq_length=max_seq_length,
    count_tasks=count_tasks,
    binary_tasks=binary_tasks,
    atom_types_list=atom_types_list_train, 
)

# åˆ›å»ºéªŒè¯é›†æ•°æ®é›†
val_dataset = SpectraDataset(
    ir_spectra=val_ir_spe_filtered,
    uv_spectra=val_uv_spe_filtered,
    c_spectra=val_nmrc_spe_filtered,
    h_spectra=val_nmrh_spe_filtered,
    high_mass_spectra=val_high_mass_spe,
    smiles_list=val_smiles_list,
    auxiliary_data=val_aux_data,
    char2idx=char2idx,
    max_seq_length=max_seq_length,
    count_tasks=count_tasks,
    binary_tasks=binary_tasks,
    atom_types_list=atom_types_list_val, 
)

# åˆ›å»ºæµ‹è¯•é›†æ•°æ®é›†
test_dataset = SpectraDataset(
    ir_spectra=test_ir_spe_filtered,
    uv_spectra=test_uv_spe_filtered,
    c_spectra=test_nmrc_spe_filtered,
    h_spectra=test_nmrh_spe_filtered,
    high_mass_spectra=test_high_mass_spe,
    smiles_list=test_smiles_list,
    auxiliary_data=test_aux_data,
    char2idx=char2idx,
    max_seq_length=max_seq_length,
    count_tasks=count_tasks,
    binary_tasks=binary_tasks,
    atom_types_list=atom_types_list_val, 
)


from torch.utils.data import DataLoader

# åˆ›å»ºè®­ç»ƒé›†æ•°æ®åŠ è½½å™¨
train_dataloader = DataLoader(
    train_dataset, 
    batch_size=128,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

# åˆ›å»ºéªŒè¯é›†æ•°æ®åŠ è½½å™¨
val_dataloader = DataLoader(
    val_dataset,
    batch_size=128, 
    shuffle=False,
    num_workers=4,
    drop_last=True,
)

# åˆ›å»ºæµ‹è¯•é›†æ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
test_dataloader = DataLoader(
    test_dataset,
    batch_size=1,
    shuffle=False,
    num_workers=0
)





# è®¡ç®—æ¯ä¸ªè®¡æ•°ä»»åŠ¡çš„ç±»åˆ«æ•°
count_task_classes = {}
for task in count_tasks:
    max_value = int(auxiliary_data[task].max())
    count_task_classes[task] = max_value + 1  # ç±»åˆ«æ•°



# # å®ä¾‹åŒ–æ¨¡å‹æ—¶ï¼Œä¼ é€’ count_task_classes
# model = AtomPredictionModel(vocab_size, count_task_classes, binary_tasks).to(device)


def load_model(model_path, vocab_size, char2idx):
    """
    åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    """
    # åˆå§‹åŒ–æ¨¡å‹
    model = AtomPredictionModel(vocab_size=vocab_size, count_tasks_classes=None, binary_tasks=None)
    model.to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    return model


# def init_weights(m):
#     if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):
#         nn.init.xavier_uniform_(m.weight)
#         if m.bias is not None:
#             nn.init.zeros_(m.bias)
#     elif isinstance(m, nn.Embedding):
#         nn.init.uniform_(m.weight, -0.1, 0.1)
# model.apply(init_weights)

 # å®šä¹‰æ¨¡å‹æ–‡ä»¶è·¯å¾„
model_path = '/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/weights_scaffold_at/0806_ft.pth'

# åŠ è½½æ¨¡å‹
model = load_model(model_path, vocab_size, char2idx)

# criterion = ContrastiveLoss()
# criterion = AtomPredictionLoss()
ignore_index = char2idx['<PAD>']
criterion = SMILESLoss(ignore_index)

count_task_loss_fn = nn.CrossEntropyLoss()
binary_task_loss_fn = nn.BCEWithLogitsLoss()

# å®šä¹‰ä¼˜åŒ–å™¨ï¼Œä»…ä¼˜åŒ–æ–°å¢çš„ count_task_heads å’Œ binary_task_heads
# trainable_params = [param for name, param in model.named_parameters() if param.requires_grad]
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)



# é€‰æ‹©è®­ç»ƒæ–¹å¼
use_semantic_supervision = True  # è®¾ç½®ä¸ºTrueä½¿ç”¨è¯­ä¹‰ç›‘ç£ï¼ŒFalseä½¿ç”¨åŸå§‹è®­ç»ƒ

if use_semantic_supervision:
    # ä½¿ç”¨è¯­ä¹‰ç›‘ç£çš„æŸå¤±å‡½æ•°
    ignore_index = char2idx['<PAD>']
    semantic_criterion = SemanticSupervisedSMILESLoss(
        ignore_index=ignore_index, 
        aux_weight=0.2,  # è¾…åŠ©ä»»åŠ¡æƒé‡
        count_weight=1.0,  # è®¡æ•°ä»»åŠ¡æƒé‡
        binary_weight=1.0  # äºŒå…ƒä»»åŠ¡æƒé‡
    )
    
    print("ğŸš€ å¼€å§‹ä½¿ç”¨è¯­ä¹‰ç›‘ç£çš„è®­ç»ƒ...")
    print(f"ğŸ“Š é‡è¦çš„è®¡æ•°ä»»åŠ¡: {semantic_criterion.important_count_tasks}")
    print(f"ğŸ”¢ é‡è¦çš„äºŒå…ƒä»»åŠ¡: {semantic_criterion.important_binary_tasks}")
    
    # Train the model with semantic supervision
    train_with_semantic_supervision(
        model,
        semantic_criterion,
        optimizer,
        train_dataloader,
        val_dataloader,
        epochs=1000,
        save_dir=f'/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/weights_{data_split_mode}_semantic',
        use_adaptive_weight=True  # ä½¿ç”¨è‡ªé€‚åº”æƒé‡è°ƒæ•´
    )
else:
    # ä½¿ç”¨åŸå§‹è®­ç»ƒå‡½æ•°ï¼ˆä»…SMILESæŸå¤±ï¼‰
    print("ğŸ“š ä½¿ç”¨åŸå§‹è®­ç»ƒæ–¹å¼ï¼ˆä»…SMILESæŸå¤±ï¼‰...")
    train(
        model,
        criterion,
        optimizer,
        train_dataloader,
        val_dataloader,
        epochs=1000,
        save_dir=f'/data4/linkaiqing/sm_pretrained/fangyang/gp/csv/weights_{data_split_mode}_original'
    )
